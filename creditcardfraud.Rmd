---
title: "HarvardX: PH125.9x Data Science  \n   Choose your own Project"
author: "Subrahmanyam  Aryasomayajula"
date: "October 6, 2022"
output: pdf_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE , warning = FALSE, message = FALSE)
```


### Introduction

Credit Card Fraud detection  project is done as a part of Choose your Own (CYO) Data Science Project , HarvardX: PH125.9x.
In the project we have obtained creditcard.csv data from kaggle dataset . The data has been inspected and models
have been applied to evaluate them for the accuracy of predicting the faults in the creditcard data set .
This course also  refers to and draws in on the knowledge developed as a part of 
"Data Science professional certificate program"  by Harvardx.


###  Overview 
In this project we evaluate  K-Nearest Neighbors , Naive Bayes classifying algorithms, Random Forest  models. Models have been trained 
and tested on the the Credit Card Fraud Detection data set to  discover the highest accuracy model between the three models. 
The credit card transactions data set obtained from the kaggle  has been randomly sampled to obtain a subset of the data . 
The data has been further divided into training and test data sets to  build a training model and test the data .
Confusion matrix has been generated in testing phase to obtain the metrics to evaluate the accuracy of the models. 


###  Executive Summmary

The goal of the project is to apply each of the three models K-Nearest Neighbours , Naive Bayes classifying algorithms, Random Forest 
on the Credit Card Fraud Detection dataset and calculate the confusion matrix from which we can determine the accuracy of each of the 
three models .

confusion matrix measures the following four values:


                           **Actually Negactive(0)       Actually Positive (1) **
**Predicted Negative (0)**     True  Negative (TN)       False  Negative (FN)               
**Predicted Positive (1)**     False  Positive (FP)      True  Positive  (TP)


Total number of  True Negative (TN) are the transaction detected as not fraudulent but in reality are also not fraudulent.
Total number of False Negative (FN) are  the transactions detected as  not fraudulent  but in reality are fraudulent .
Total number of False Positive (FP) are the transaction detected as fradulent  but are  not fraudulent in reality .
Total number of True Positives (TP)  are the  transaction detected as   fradulent   and really are fraudulent .



```{r partition-data, eval=TRUE}

library(caret)
library(class)
library(e1071)
library(ROCR)
library(dplyr)

library(ggplot2) 
library(readr) 
library(pROC) 
library(randomForest)
library(corrplot) 

library(data.table)
library(rpart)
library(stringr)
library(rmarkdown)
library(tinytex)
library(knitr)

tinytex::install_tinytex(force = TRUE)

dir = getwd()

dir = getwd()

download.file("https://github.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/raw/master/creditcard.csv","./creditcard.csv" )

filename = paste(dir , "/creditcard.csv",sep ="")


filename  = str_replace_all(filename , "/","\\\\")
credit <- read.csv(filename)

```



#column names , types and sample records examined.

```{r , EVAL = TRUE}
str(credit)
```


#Dimensions of the data frame i.e total number of rows and columns .

```{r , EVAL = TRUE}
dim(credit)
```

#[1] 284807     31




#factor transformation was on variable class .: 

```{r , EVAL = TRUE}
credit$Class <- factor(credit$Class)
```

#Summary stats  on every columns on the dataframe .

```{r , EVAL = TRUE}
summary(credit)

```


#Top 5 records

```{r , EVAL = TRUE}
head(credit)
```

###  Dataset preparation



The data set is large to execute in reasonable time on a 16b ram , Intel 7th generation laptop . Hence
20% of the data was randomly sampled. Training and test data sets were generated from randomized data.
Training data Set is 75% of sample 
Test data set is 25% of sample 

set seed for random sampling  and take 20% sample data approximately.

```{r , EVAL = TRUE}

set.seed(2000)

samp <- sample(1:nrow(credit), round(0.2*nrow(credit)))

credit <- credit[samp, ]

```

```{r , EVAL = TRUE}

nrow(credit)
```

```{r , EVAL = TRUE}

index <- createDataPartition(credit$Class, p = 0.75, list = F)

```

Training data Set 75% of sample

```{r , EVAL = TRUE}
train <- credit[index, ]
```

```{r , EVAL = TRUE}
nrow(train)
```


Test data set 25% of sample

```{r , EVAL = TRUE}
test <- credit[-index, ]
nrow(test)
```


# K-Nearest Neighbors
The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. It can be used for either regression or classification problems,
it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.
In order to determine which data points are closest to a given query point, the distance between the query point and the other data points will need to be calculated. These distance metrics help to form decision boundaries, which partitions query points into different regions.
Euclidean distance,Manhattan distance,Minkowski distance ,Hamming distance  are various ways of measuring the distance.

The k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset.


In our credit card fraud detection data set ,as all the variables were of class either "numeric" or "integer" ,
The knn classification with  the number of neighbours was set to 5 as a default. 

```{r , EVAL = TRUE}

knn1 <- knn(train = train[,-31], test = test[,-31], cl = train$Class, k = 5)

cmknn <- confusionMatrix(knn1, test$Class, positive = "1")
cmknn

```


In Model the number of nearest neighbours was set to 5.
`r cmknn$overall["Accuracy"]*100`%  Accuracy in prediction was acheived as per the above confusion matrix .
Out of a total `r nrow(test)` test cases , 


Total number of  True Negative (TN) are   `r cmknn$table[1,1] `   the transaction detected as not fraudulent but in reality are also not fraudulent.
Total number of False Negative (FN) are   `r cmknn$table[1,2] `   the transactions detected as  not fraudulent  but in reality are fraudulent .
Total number of False Positive (FP) are   `r cmknn$table[2,1] `   the transaction detected as fradulent  but are  not fraudulent in reality .
Total number of True Positives (TP) are   `r cmknn$table[2,2] `   the  transaction detected as   fradulent   and really are fraudulent .




Although `r cmknn$overall["Accuracy"]*100`%   accuracy was obtained with the specified index of k = 5 , there are still some  shortcomings as seen
from   "confusionMatrix" output .The model has not predicted any True Postive cases as illustrated by the  class = 1 , prediction is 0 .


# Naive Bayes

Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable.

The Naive Bayes classification algorithm is a probabilistic classifier. It is based on probability models that incorporate strong independence assumptions.The independence assumptions often do not have an impact on reality. Therefore they are considered as naive.

We can derive probability models by using Bayes' theorem . Depending on the nature of the probability model, we can train the Naive Bayes algorithm in a supervised learning setting.

A Naive Bayes model consists of a large cube that includes the following dimensions:
Input field value for discrete fields, or input field value range for continuous fields.
Continuous fields are divided into discrete bins by the Naive Bayes algorithm

Target field value ,means that a Naive Bayes model records how often a target field value appears together with a value of an input field.


Naive Bayes model based  analysis of the data set to obtain the confusion matrix for fault detection. 

The model was to adjust  for the possibility of experiencing posterior class probability of "0" by "laplace = 1".

```{r , EVAL = TRUE}
bayes <- naiveBayes(Class~., data = train, laplace = 1)
bayes$apriori

pred <- predict(bayes, test)
cmnb <- confusionMatrix(pred, test$Class, positive = "1")
cmnb
```


`r cmnb$overall["Accuracy"]*100`%  Accuracy in prediction was acheived as per the above confusion matrix .
Out of a total `r nrow(test)` test cases , 

Total number of  True Negative (TN) are `r cmnb$table[1,1] `  the transaction detected as not fraudulent but in reality are also not fraudulent.
Total number of False Negative (FN) are `r cmnb$table[1,2] `  the transactions detected as  not fraudulent  but in reality are fraudulent .
Total number of False Positive (FP) are `r cmnb$table[2,1] `  the transaction detected as fradulent  but are  not fraudulent in reality .
Total number of True Positives (TP)  are `r cmnb$table[2,2]`  the  transaction detected as   fradulent   and really are fraudulent .



Naive Bayes is under performing for this set .



Naive Bayes is an underperforming algorithm in comparison with knn  as we can see the accuracy is `r cmnb$overall["Accuracy"]*100`%   with Naive Bayes Vs 
`r cmknn$overall["Accuracy"]*100`%   for K Nearest  neighbours .




# Build the Model with the Random Forest with decision trees set to 40.

Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our modelâ€™s prediction.
The fundamental idea behind random forests is that a large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models .The low correlation between models is the key  .
uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this positive effect is that the trees protect each other from their individual errors . While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction.

For our credit card data set we have used number of decision trees to 40.

```{r , EVAL = TRUE}
random_model_train <- randomForest(Class ~ ., data = train,ntree = 40)

random_pred_test <- predict(random_model_train,test)

cmrf <-  confusionMatrix(random_pred_test,test$Class ,positive = "1") 
cmrf

```



`r cmrf$overall["Accuracy"]*100`%  Accuracy in prediction was acheived as per the above confusion matrix .
Out of a total `r nrow(test)` test cases , 


Total number of  True Negative (TN) are  `r cmrf$table[1,1] `  the transaction detected as not fraudulent but in reality are also not fraudulent.
Total number of False Negative (FN) are  `r cmrf$table[1,2] `  the transactions detected as  not fraudulent  but in reality are fraudulent .
Total number of False Positive (FP) are  `r cmrf$table[2,1] `  the transaction detected as fradulent  but are  not fraudulent in reality .
Total number of True Positives (TP)  are `r cmrf$table[2,2] `  the  transaction detected as   fradulent   and really are fraudulent .




Over all Random forest is performing the best with `r cmrf$overall["Accuracy"]*100`%  accuracy compared to Naive Bayes which is an
under performing algorithm in comparison with knn .  We can see the accuracy is `r cmnb$overall["Accuracy"]*100`%   with Naive Bayes and then 
`r cmknn$overall["Accuracy"]*100`%   for K Nearest  neighbors.




# Conclusion

In conclusion , with regards to creditcard data set, Random forest is better performing than K-Nearest Neighbors
algorithm and Naive Bayes in the extraction of the most accurate predictions in terms of whether a credit card will
be detected fraud or not. 
 
Random forest has detected `r cmrf$table[2,2]`  True positives , in comparison to `r cmnb$table[2,2]` from Naive Bayes , `r cmknn$table[2,2]` from K nearest neighbors .
Although we  see Naive Bayes has detected `r cmnb$table[2,2]` True positives , we should also consider false negatives and false positives
to evaluate the accuracy of the algorithm . Random Forest having `r cmrf$table[2,1]`  False  positives and `r cmrf$table[1,2]`  false negatives, vs  Naive Bayes
having `r cmnb$table[2,1]` False positives and  `r  cmnb$table[1,2]` False negatives , and K- Nearest Neighbors having `r cmknn$table[2,1]` false positives and 
`r  cmknn$table[1,2]`  false negatives .This is the reason for overall accuracy of the   Random forest   the best with 99.93 accuracy compared to Naive Bayes which is an
under performing algorithm  with 98.08 and then 99.83 for K Nearest  neighbors.

However the performance of the Random forest on high volume datasets can be significantly slow .

